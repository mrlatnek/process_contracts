{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summariza Sales Contracts \n",
    "\n",
    "<b>Objective:</b> The goal of this project is to identify the amount of soft versus hard information in SEC filings of sales contracts that summarize a sales contract. Soft information is often forward-looking, uncertain, or qualitative. Hard information is often specific, historical, certain, and quantitative. \n",
    "\n",
    "<b>Input Data:</b> These transcripts are probably not in standardized format. But, to goal is to extract the content of information disseminated publicly through webcasts or press releases, so please process all text in each transcript or press release. Please remove boilerplate safe harbor statement and company description, and header indices created by the transcript or newswire companies. Each transcript or press release is in a separate .txt file with a unique identifier as the file name. \n",
    "\n",
    "<b>Output items:</b>\n",
    "\n",
    "•\tTotal_words\t\n",
    "•\tNumber_Entities\t\n",
    "•\tWords_in_Entities\t\n",
    "•\tNumber_of_Times\t\n",
    "•\tWords_in_Times\t\n",
    "•\tNumber_of_Locations\t\n",
    "•\tWords_in_Locations\t\n",
    "•\tNumber_of_Organizations\t\n",
    "•\tWords_in_Organizations\t\n",
    "•\tNumber_of_Persons\t\n",
    "•\tWords_in_Persons\t\n",
    "•\tNumber_of_Money\t\n",
    "•\tWords_in_Money\t\n",
    "•\tNumber_of_Percentages\t\n",
    "•\tWords_in_Percentages\t\n",
    "•\tNumber_of_Dates\t\n",
    "•\tWords_in_Dates\n",
    "\n",
    "•\tNumber of forward-looking words (Bozanic Roulstone Buskirk 2016 Appendix A word list)\n",
    "\n",
    "•\tNumber of uncertain words (Bozanic et al. 2018 use Loughran and McDonald’s uncertainty measure)\n",
    "\n",
    "•\tNumber of positive words (Harvard dictionary)\n",
    "\n",
    "•\tNumber of negative words (Harvard dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "# Third Party Libraries\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Name Entity Recognitation\n",
    "# https://juejin.im/post/5971a4b9f265da6c42353332?utm_source=gold_browser_extension%5D\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Measure the Readability\n",
    "# https://pypi.org/project/textstat/\n",
    "import textstat\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import unicodedata\n",
    "\n",
    "# Measure the Sentiment \n",
    "# https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "from textblob import TextBlob\n",
    "\n",
    "REGEX = r'\\d{5} \\d+/\\d+/\\d+ \\d+/\\d+/\\d+'\n",
    "TAG = r'<(.*?)>(.*?)</(.*?)>'\n",
    "\n",
    "# Measure the forward looking statements\n",
    "LINES = [temp.strip() for temp in open('expressions.txt', 'r').readlines()]\n",
    "FWD_REGEX = re.compile(r'%s' % (r'\\b' + r'\\b|\\b'.join(LINES) + r'\\b'),\n",
    "                   re.IGNORECASE)\n",
    "IGNORE = ['call', r'questions?', 'press release', 'slides?', 'webcast',\n",
    "          r'\\?', r'(can|do|will|have) you', r'Q ?:', r'\\[Q', r'\\[?Operator\\]?']\n",
    "REG_IGNORE = re.compile(r'%s' %  r'|'.join(IGNORE), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Write a python list into person.csv file\n",
    "\n",
    "import csv\n",
    "csvData = [['Person', 'Age'], ['Peter', '22'], ['Jasmine', '21'], ['Sam', '24']]\n",
    "with open('person.csv', 'w',newline='') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def get_content(filein):\n",
    "    \"Get the main content of each investor meeting disclosure\"\n",
    "    #current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    #folder = os.path.join(current_dir, 'Factiva week 1&2_v2')\n",
    "    \n",
    "    #path = os.path.join(folder, filein)\n",
    "    \n",
    "    data = open(filein, 'r',encoding='utf-8',errors=\"surrogateescape\")\n",
    "\n",
    "    mess = data.read()\n",
    "    \n",
    "    # presentation = 1 means it is a investor meeting including some conversations\n",
    "    # presentation = 0 means it is a press releases that only include a summary\n",
    "    \n",
    "    presentation = 1\n",
    "    \n",
    "    if '\\x00' in mess:\n",
    "        data = open(path, 'r',encoding='utf-16')\n",
    "        mess = data.read()\n",
    "        \n",
    "    if re.findall(r'[^\\|] [A-Z]+: \\w',mess) == []:\n",
    "        presentation = 0\n",
    "    \n",
    "    # make the '\\n' optional to include the missing ones and some company contact info as well as the copayright statement\n",
    "    mess = mess.strip()\n",
    "    content = re.findall(r'([A-Z][a-z].+)\\.?\\n?',mess)\n",
    "    \n",
    "    content = ' '.join(content)\n",
    "    content = re.sub(r\"[\\x97\\x95\\xa0]\",\" \", content)\n",
    "    content = re.sub(r\"\\\\\" + \"'\",\"\", content)\n",
    "    \n",
    "    content = unicodedata.normalize(\"NFKD\", content)\n",
    "    content = preprocess_text(content)\n",
    "    #content = content.strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return content,presentation\n",
    "\n",
    "#This is an example of a general description of the company as a press release classified as 0\n",
    "# get_content('73681_11272012.txt')\n",
    "# get_content('23297_09252003.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"Get a list of tokens (words) for a given text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered = [i for i in tokens if not all(j in string.punctuation for j in i)]\n",
    "    final = [w.upper() for w in filtered if not w in stop_words]\n",
    "    \n",
    "    return final\n",
    "\n",
    "# get_tokens(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(headline=True):\n",
    "    \"\"\"Split the document file into articles.\"\"\"\n",
    "    path = os.path.join(current_dir, '2.Disclosure_sample.txt')\n",
    "    lines = codecs.open(path, 'rU', 'latin').readlines()\n",
    "    docs = list(dump_splitter(lines, headline=headline))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_splitter(data, headline=True):\n",
    "    \"\"\"Generator of article chunks.\"\"\"\n",
    "    buff = []\n",
    "    for line in data:\n",
    "        if re.findall(REGEX, line):\n",
    "            if buff:\n",
    "                if not headline:\n",
    "                    buff.pop(1)\n",
    "                yield u' '.join(buff)\n",
    "                buff[:] = []\n",
    "        if line.strip():\n",
    "            buff.append(line.strip())\n",
    "    yield u' '.join(buff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    "    \"\"\"Preprocess text.\"\"\"\n",
    "    # Extract preamble\n",
    "    #preamble = re.findall(REGEX, doc)[0].split()\n",
    "    text = re.sub(REGEX, '', doc).strip()\n",
    "\n",
    "    # Remove irrelevant text\n",
    "    text = re.sub((r'. (More information|For information on|For more '\n",
    "                   'information) .*?$'), '', text)\n",
    "\n",
    "    # Titlecase uppercase headlines\n",
    "    capital = ''\n",
    "    for char in text:\n",
    "        if char.isupper() or char in string.punctuation + ' ':\n",
    "            capital += char\n",
    "        else:\n",
    "            break\n",
    "    if len(capital.split()) > 3:\n",
    "        text = text.replace(capital, capital.title())\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readability(text):\n",
    "    \"Calculate some readability measures from the textstat package \"\n",
    "    \n",
    "    #Return the Flesch Reading Ease Score\n",
    "    read_ease = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    #Return the Fog Index Grade\n",
    "    read_grade = textstat.gunning_fog(text)\n",
    "    \n",
    "    return read_ease,read_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_fwd(sentence):\n",
    "    \"\"\"Return true if the sentece is a fwd looking statement.\"\"\"\n",
    "    if sentence.isupper():\n",
    "        return False\n",
    "    if REG_IGNORE.search(sentence):\n",
    "        return False\n",
    "    return bool(FWD_REGEX.search(sentence))\n",
    "\n",
    "is_fwd(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now we will move to page 21.',\n",
       " \"And I'm going to ask Drew to go over the pro forma financial impact.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"Sentence tokenizer.\"\"\"\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return sent_detector.tokenize(text.strip())\n",
    "\n",
    "get_sentences(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 0.5, ['Now we will move to page 21.'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fwd_statements(text):\n",
    "    \"\"\"Get number of forward-looking statements.\"\"\"\n",
    "    all_sents = get_sentences(text)\n",
    "    \n",
    "    len_all = len(all_sents)\n",
    "    if not len_all:\n",
    "        return None, None, None\n",
    "    fwd = 0\n",
    "    fwd_sents = []\n",
    "    for sent in all_sents:\n",
    "        if is_fwd(sent):\n",
    "            fwd += 1\n",
    "            fwd_sents.append(sent)\n",
    "    return len_all, fwd, fwd * 1.0 / len_all, fwd_sents\n",
    "\n",
    "get_fwd_statements(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(text):\n",
    "    \"Count and sum the number of words in each entity and the number of entities \"\n",
    "    if len(text)>= 1000000:\n",
    "        print('len of text exceeds ', len(text))\n",
    "        text = text[:999999]\n",
    "        \n",
    "    text = nlp(text)\n",
    "\n",
    "    labels = set([w.label_ for w in text.ents])\n",
    "    entity_results = dict()\n",
    "    word_results = dict()\n",
    "\n",
    "    for label in labels:\n",
    "        entities = [e.string for e in text.ents if label==e.label_]\n",
    "        \n",
    "        #get the number of words\n",
    "        entity_list = \" \".join(entities).strip()\n",
    "        tokens = word_tokenize(entity_list)\n",
    "        word_results[label] = len(tokens)\n",
    "        \n",
    "        #get the number of entities\n",
    "        entity_results[label] = len(entities)\n",
    "\n",
    "\n",
    "    for cat in ['TIME','LOC','ORG','PERSON','MONEY','PERCENT','DATE']:\n",
    "            if not cat in entity_results.keys():\n",
    "                entity_results[cat] = 0\n",
    "            if not cat in word_results.keys():\n",
    "                word_results[cat] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    total_entities = sum(entity_results.values())\n",
    "    e_times = entity_results['TIME']\n",
    "    e_locations = entity_results['LOC']\n",
    "    e_organizations = entity_results['ORG']\n",
    "    e_persons = entity_results['PERSON']\n",
    "    e_money = entity_results['MONEY']\n",
    "    e_percentages = entity_results['PERCENT']\n",
    "    e_dates = entity_results['DATE']\n",
    "    \n",
    "    total_entity_words = sum(word_results.values())\n",
    "    w_times = word_results['TIME']\n",
    "    w_locations = word_results['LOC']\n",
    "    w_organizations = word_results['ORG']\n",
    "    w_persons = word_results['PERSON']\n",
    "    w_money = word_results['MONEY']\n",
    "    w_percentages = word_results['PERCENT']\n",
    "    w_dates = word_results['DATE']\n",
    "        \n",
    "    \n",
    "    return total_entities, e_times, e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "    total_entity_words, w_times, w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_uncertainty(text):\n",
    "    \"Count the frequencies of uncertain words from a list stored in uncertainty text file\"\n",
    "    \n",
    "    cnt = Counter()\n",
    "    wanted = re.findall('\\w+',open('LM_uncertainty.txt').read())\n",
    "    words = get_tokens(text)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wanted:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    total_cnt = sum(dict(cnt).values())\n",
    "    \n",
    "    return total_cnt,cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sentiments(text):\n",
    "    \"Count the number of positive and negative words based off the LoughranMcDonald_SentimentWordLists_2018\"\n",
    "  \n",
    "    cnt_pos = Counter()\n",
    "    cnt_neg = Counter()\n",
    "    words = get_tokens(text)\n",
    "    \n",
    "    wanted_pos = re.findall('\\w+',open('LM_positive.txt').read())\n",
    "    wanted_neg = re.findall('\\w+',open('LM_negative.txt').read())\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wanted_pos:\n",
    "            cnt_pos[word] += 1\n",
    "        elif word in wanted_neg:\n",
    "            cnt_neg[word] += 1\n",
    "            \n",
    "    pos = sum(dict(cnt_pos).values())\n",
    "    neg = sum(dict(cnt_neg).values())\n",
    "    \n",
    "    return pos,neg,cnt_pos,cnt_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tERROR for file all_texts/694710.txt: 0 length\n",
      "\tERROR for file all_texts/721313.txt: 0 length\n",
      "\tERROR for file all_texts/724140.txt: 0 length\n",
      "\tERROR for file all_texts/937676.txt: 0 length\n",
      "\tERROR for file all_texts/1125157.txt: 0 length\n",
      "\tERROR for file all_texts/1105592.txt: 0 length\n",
      "\n",
      "All done.\n",
      " These text files do not have required contents:\n",
      "\n",
      "['all_texts/694710.txt', 'all_texts/721313.txt', 'all_texts/724140.txt', 'all_texts/937676.txt', 'all_texts/1125157.txt', 'all_texts/1105592.txt']\n"
     ]
    }
   ],
   "source": [
    "#use this cell for processing transcripts\n",
    "import csv\n",
    "\n",
    "results = []\n",
    "with open(\"Input_transcript.csv\", encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        results.append(row)\n",
    "\n",
    "#create the header\n",
    "header = [['eventid', 'year', 'companyName', 'companyTicker', 'startDate', 'eventTitle',\n",
    "           'Content Format:\\nInvestor Meeting - 1\\nPress Release - 0',\n",
    "           'Total Words',\n",
    "           'Flesch Reading Ease Score','Gunning Fog Index',\n",
    "           'Number of Entities','Words in Entities',\n",
    "           'Number of Times','Words in Times',\n",
    "           'Number of Locations','Words in Locations',\n",
    "           'Number of Organizations','Words in Organizations',\n",
    "           'Number of Persons','Words in Persons',\n",
    "           'Number of Money','Words in Money',\n",
    "           'Number of Percentages','Words in Percentages',\n",
    "           'Number of Dates','Words in Dates',\n",
    "           'Total Sentences', 'Total Forward Sentences', 'Forward Ratio',\n",
    "           'Uncertainty Words',\n",
    "           'Polarity','Subjectivity',\n",
    "           'Positive Words','Negative Words'\n",
    "           ]]\n",
    "\n",
    "missing = []\n",
    "#open a new output csv file \n",
    "with open('Investor_Meeting_Transcripts_new.csv', 'w',newline='') as fileout:\n",
    "    writer = csv.writer(fileout)\n",
    "    writer.writerows(header)\n",
    "    #print ('\\nScanning %d files in \"%s\"\\n' % (len(files), folder))\n",
    "    \n",
    "    for kk in results[1:]:\n",
    "        \n",
    "        #Check the current file in the process\n",
    "#         print(filein)\n",
    "        \n",
    "#         if not (num + 1) % 1000:\n",
    "#             secs = time.time() - start\n",
    "#             print ('\\tFile %d done (%d secs)' % (num + 1, secs))\n",
    "#             start = time.time()\n",
    "        filein = 'all_texts/' + kk[0] + '.txt'\n",
    "        with open(filein, 'w', encoding='utf-8') as text_file:\n",
    "            text_file.write(kk[-1])\n",
    "\n",
    "            \n",
    "        #ID,Date = filein.strip('.txt').split('_')\n",
    "        \n",
    "        #Date = datetime.datetime.strptime(Date,'%m%d%Y').strftime('%Y-%m-%d')\n",
    "    \n",
    "        eventid, year, companyName, companyTicker, startDate, eventTitle = kk[0], kk[1],  kk[2], kk[3], kk[4], kk[5] \n",
    "        \n",
    "        content,presentation = get_content(filein)\n",
    "        \n",
    "        words = len(get_tokens(content))\n",
    "        \n",
    "        #get the readability measures\n",
    "        read_ease,read_grade = get_readability(content)\n",
    "        \n",
    "        if content == '':\n",
    "            missing.append(filein)\n",
    "            print ('\\tERROR for file %s: 0 length' % filein)\n",
    "            continue\n",
    "            \n",
    "        total_entities,e_times,e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "        total_entity_words,w_times,w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates,\\\n",
    "        = get_results(content)\n",
    "\n",
    "        fwd = get_fwd_statements(content)\n",
    "        \n",
    "        #count the frequencies of uncertain words \n",
    "        total_cnt,cnt = get_uncertainty(content)\n",
    "        \n",
    "        #use textblob package to analyze the sentiment\n",
    "        blob = TextBlob(content)\n",
    "        polarity = round(blob.sentiment.polarity,2)\n",
    "        subjectivity = round(blob.sentiment.subjectivity,2)\n",
    "        \n",
    "        #count the positive and negative words\n",
    "        pos, neg, cnt_pos, cnt_neg = get_sentiments(content)\n",
    "\n",
    "#             if verbose:\n",
    "#                 os.system('clear')\n",
    "#                 print ('\\n%d)' % num)\n",
    "#                 for sent in res[3]:\n",
    "#                     print ('\\t%s' % sent)\n",
    "#                 _ = raw_input('')\n",
    "\n",
    "        row = [eventid, year, companyName, companyTicker, startDate, eventTitle, presentation, words,\\\n",
    "            read_ease,read_grade,\\\n",
    "            total_entities, total_entity_words, e_times, w_times, e_locations, w_locations, \\\n",
    "            e_organizations, w_organizations, e_persons, w_persons, e_money, w_money, \\\n",
    "            e_percentages, w_percentages, e_dates, w_dates,\\\n",
    "            fwd[0], fwd[1], round(fwd[2],2),\\\n",
    "            total_cnt,\\\n",
    "            polarity, subjectivity,\\\n",
    "            pos,neg]\n",
    "\n",
    "        writer.writerow(row)\n",
    "        \n",
    "    print('\\nAll done.\\n These text files do not have required contents:\\n')\n",
    "    print(missing)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type NoneType doesn't define __round__ method\n",
      "type NoneType doesn't define __round__ method\n",
      "type NoneType doesn't define __round__ method\n",
      "type NoneType doesn't define __round__ method\n",
      "type NoneType doesn't define __round__ method\n",
      "type NoneType doesn't define __round__ method\n",
      "type NoneType doesn't define __round__ method\n",
      "\n",
      "All done.\n",
      " These text files do not have required contents:\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#use this for processing sales contracts\n",
    "with open('sales_contracts.csv') as f:\n",
    "    sales = [s for line in f.readlines() for s in line[:-1].split(',')]\n",
    "sales = sales[1:] \n",
    "#create the header\n",
    "header = [[ 'Total Words',\n",
    "           'Flesch Reading Ease Score','Gunning Fog Index',\n",
    "           'Number of Entities','Words in Entities',\n",
    "           'Number of Times','Words in Times',\n",
    "           'Number of Locations','Words in Locations',\n",
    "           'Number of Organizations','Words in Organizations',\n",
    "           'Number of Persons','Words in Persons',\n",
    "           'Number of Money','Words in Money',\n",
    "           'Number of Percentages','Words in Percentages',\n",
    "           'Number of Dates','Words in Dates',\n",
    "           'Total Sentences', 'Total Forward Sentences', 'Forward Ratio',\n",
    "           'Uncertainty Words',\n",
    "           'Polarity','Subjectivity',\n",
    "           'Positive Words','Negative Words'\n",
    "           ]]\n",
    "\n",
    "\n",
    "\n",
    "missing = []\n",
    "#open a new output csv file \n",
    "m = 1\n",
    "with open('sales_contracts_results.csv', 'w',newline='') as fileout:\n",
    "    writer = csv.writer(fileout)\n",
    "    writer.writerows(header)\n",
    "    #print ('\\nScanning %d files in \"%s\"\\n' % (len(files), folder))\n",
    "    \n",
    "    for url in sales:\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            page = BeautifulSoup(res.text)\n",
    "            raw_text = page.body.get_text(\" \", strip = True)\n",
    "            filein = 'all_texts_asu/' + str(m)\n",
    "            m+=1\n",
    "            with open(filein, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(raw_text)\n",
    "\n",
    "            content,presentation = get_content(filein)\n",
    "\n",
    "            words = len(get_tokens(content))\n",
    "\n",
    "            #get the readability measures\n",
    "            read_ease,read_grade = get_readability(content)\n",
    "\n",
    "\n",
    "            total_entities,e_times,e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "            total_entity_words,w_times,w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates,\\\n",
    "            = get_results(content)\n",
    "\n",
    "            fwd = get_fwd_statements(content)\n",
    "\n",
    "            #count the frequencies of uncertain words \n",
    "            total_cnt,cnt = get_uncertainty(content)\n",
    "\n",
    "            #use textblob package to analyze the sentiment\n",
    "            blob = TextBlob(content)\n",
    "            polarity = round(blob.sentiment.polarity,2)\n",
    "            subjectivity = round(blob.sentiment.subjectivity,2)\n",
    "\n",
    "            #count the positive and negative words\n",
    "            pos, neg, cnt_pos, cnt_neg = get_sentiments(content)\n",
    "\n",
    "\n",
    "            row = [words,\\\n",
    "                read_ease,read_grade,\\\n",
    "                total_entities, total_entity_words, e_times, w_times, e_locations, w_locations, \\\n",
    "                e_organizations, w_organizations, e_persons, w_persons, e_money, w_money, \\\n",
    "                e_percentages, w_percentages, e_dates, w_dates,\\\n",
    "                fwd[0], fwd[1], round(fwd[2],2),\\\n",
    "                total_cnt,\\\n",
    "                polarity, subjectivity,\\\n",
    "                pos,neg]\n",
    "\n",
    "            writer.writerow(row)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            writer.writerow([0]*27)\n",
    "        \n",
    "    print('\\nAll done.\\n These text files do not have required contents:\\n')\n",
    "    print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
